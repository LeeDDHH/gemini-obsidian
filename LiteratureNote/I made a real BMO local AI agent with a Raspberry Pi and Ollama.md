# I made a real BMO local AI agent with a Raspberry Pi and Ollama
https://www.youtube.com/watch?v=l5ggH-YhuAw

## **요약**

- 화자는 **어드벤처 타임의 BMO(본문에서는 Beimo로 표기)**를 현실에서 구현한 **로컬(온디바이스) ‘몸을 가진 AI 에이전트’**를 만들겠다고 선언한다. 단순 챗봇이 아니라 **스스로 판단하고 행동**하며, **게임기·컴퓨터·카메라** 기능을 동시에 수행하는 존재를 목표로 한다.
    
- 하드웨어는 **Raspberry Pi 5(16GB RAM)**를 중심으로 **5인치 터치스크린(800×480), USB 마이크/스피커, Pi 카메라 모듈, 전면 2개의 USB 포트**를 구성한다. 버튼은 GPIO로 직접 연결하지 않고, **마이크로컨트롤러가 버튼 입력을 ‘키보드 입력’으로 변환**해 OS 어디서든 게임패드처럼 쓰게 만든다.
    
- 외형은 부품 치수 측정 → 2D 목업 → **PCB 설계/주문(KiCad)** → 3D 모델링(Blender) → 3D 프린팅 → 샌딩/프라이머/도색으로 완성한다. 자석으로 결합되는 구조와 교체 가능한 등/팔·다리 파츠 등 **모듈식 설계**가 핵심이다.
    
- 소프트웨어는 **상태 머신(Idle → Listening → Thinking → Speaking)**로 설계한다. 깨우기는 버튼 또는 **오픈웨이크워드(openWakeWord)**. 음성 인식은 **Whisper**, 로컬 LLM 실행은 **Ollama**, 음성 합성은 **Piper**를 사용한다. 응답 지연 문제를 해결하기 위해 **모델을 미리 메모리에 올려 유지(warm-up)**하고, 저장장치를 **SD → NVMe SSD**로 바꾸는 최적화를 진행한다.
    
- 멀티모달 모델 1개로 모두 처리하는 대신, 성능을 위해 **작은 텍스트 모델(Gemma 계열) + 이미지 분석 전용(Moondream)**처럼 **역할 분담**을 한다. 그리고 “키워드 트리거”가 아니라, LLM이 **도구 사용 여부를 스스로 결정**하는 **에이전트 루프(툴 호출 → 결과 반영 → 최종 응답)** 구조로 발전시킨다. 여기에 **RAG(검색 도구)**를 붙여 최신 정보/사실 조회를 가능하게 한다.
    
- 마지막 메시지는 “지능을 만드는 것보다, **진짜 인간다움이 무엇인지 이해하는 것**이 더 큰 도전”이라는 문제의식으로 귀결된다. 클라우드 중심 AI의 비용·저작권·가드레일 문제를 언급하며, 로컬/오픈 도구로 **다른 방향의 AI 사용 방식**을 실험하고자 한다는 태도가 드러난다.
    

  

## **핵심 포인트**

- **로컬 에이전트의 설계 철학**: “대화”가 아니라 “행동”까지 포함(툴/센서/입출력 결합)
    
- **성능 최적화의 현실**: 모델 로딩/스토리지 병목 → warm-up, NVMe, 모델 분리로 해결
    
- **윤리적 선택**: 유명 성우 목소리 딥페이크를 피하고, 로컬 TTS로 대체
    
- **확장성**: 버튼/포트/파츠 모듈화 + 추후 LLM 가속기, 배터리, 더 많은 툴 추가 가능
    

  

## **키워드 (최대 3개)**

- **로컬 AI 에이전트**
    
- **상태 머신 & 툴 호출(Agent Loop)**
    
- **Raspberry Pi 기반 임바디드 컴퓨팅**